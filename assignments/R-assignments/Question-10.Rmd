---
title: "Quesion 10"
output:
  pdf_document: default
  html_document:
    df_print: paged
  html_notebook: default
---
## Evaluating Model Performance 
The goal of evaluating a classification model is to have a better understanding of
how its performance will extrapolate to future cases. Since it is usually unfeasible
to test a still-unproven model in a live environment, we typically simulate future
conditions by asking the model to classify a dataset made of cases that resemble
what it will be asked to do in the future. By observing the learner's responses to
this examination, we can learn about its strengths and weaknesses.
Create the predicted probabilities from the SMS classifier built in Chapter 4.
Obtain the predicted probabilities

```
sms_test_prob <- predict(sms_classifier, sms_test, type = "raw")
head(sms_test_prob)
```
Combine the results into a data frame
```
sms_results <- data.frame(actual_type = sms_test_labels,
                          predict_type = sms_test_pred,
                          prob_spam = round(sms_test_prob[ , 2], 5),
                          prob_ham = round(sms_test_prob[ , 1], 5))
write.csv(sms_results, "sms_results.csv", row.names = FALSE)
```

```{r}
sms_results <- read.csv("sms_results.csv")
```

The first several test cases. Each line in this output shows the classifier's predicted probability of spam andham, which always sum up to 1 because these are the only two outcomes.  While constructing an evaluation dataset, it is important to ensure that you are using
the correct probability for the class level of interest. To avoid confusion, in the
case of a binary outcome, you might even consider dropping the vector for one
of the two alternatives.
```{r}
head(sms_results)
```
Test cases where the model is less confident
```{r}
head(subset(sms_results, prob_spam > 0.40 & prob_spam < 0.60))
```
Test cases where the model was wrong
```{r}
head(subset(sms_results, actual_type != predict_type))
```
Specifying vectors
```{r}
table(sms_results$actual_type, sms_results$predict_type)
```

Alternative solution using the formula interface 
```{r}
xtabs(~ actual_type + predict_type, sms_results)
```
Using the CrossTable function
```{r}
library(gmodels)
CrossTable(sms_results$actual_type, sms_results$predict_type)
```
Accuracy and error rate calculation
Accuracy
```{r}
(152 + 1203) / (152 + 1203 + 4 + 31)
```
Error rate
```{r}
(4 + 31) / (152 + 1203 + 4 + 31)
```
Error rate = 1 - accuracy
```{r}
1 - 0.9748201
```
Beyond accuracy: other performance measures: 
A confusion matrix is a table that categorizes predictions according to whether
they match the actual value.
The most common performance measures consider the model's ability to discern one
class versus all others. The class of interest is known as the positive class, while all
others are known as negative.
The relationship between the positive class and negative class predictions can be
depicted as a 2 x 2 confusion matrix that tabulates whether predictions fall into one
of the four categories:
• True Positive (TP): Correctly classified as the class of interest 
• True Negative (TN): Correctly classified as not the class of interest 
• False Positive (FP): Incorrectly classified as the class of interest 
• False Negative (FN): Incorrectly classified as not the class of interest

```{r}
library(caret)
confusionMatrix(sms_results$predict_type, sms_results$actual_type, positive = "spam")
```
Kappa statistic example using SMS classifier
```{r}
pr_a <- 0.865 + 0.109
pr_a
pr_e <- 0.868 * 0.888 + 0.132 * 0.112
pr_e
k <- (pr_a - pr_e) / (1 - pr_e)
k
```
Calculate kappa via the vcd package
```{r}
library(vcd)
Kappa(table(sms_results$actual_type, sms_results$predict_type))
```
Calculate kappa via the irr package
```{r}
library(irr)
kappa2(sms_results[1:2])
```
```{r}
sens <- 152 / (152 + 31)
sens
spec <- 1203 / (1203 + 4)
spec
```

Sensitivity and specificity example using SMS classifier example using the caret package
```{r}
library(caret)
sensitivity(sms_results$predict_type, sms_results$actual_type, positive = "spam")
specificity(sms_results$predict_type, sms_results$actual_type, negative = "ham")
```
Precision and recall
```{r}
prec <- 152 / (152 + 4)
prec
rec <- 152 / (152 + 31)
rec
```
Example using the caret package
```{r}
library(caret)
posPredValue(sms_results$predict_type, sms_results$actual_type, positive = "spam")
sensitivity(sms_results$predict_type, sms_results$actual_type, positive = "spam")
```
F-measure
```{r}
f <- (2 * prec * rec) / (prec + rec)
f
f <- (2 * 152) / (2 * 152 + 4 + 31)
f
```
Visualizing Performance Tradeoffs
```{r}
library(pROC)
sms_roc <- roc(sms_results$actual_type, sms_results$prob_spam)
```
ROC curve for Naive Bayes.
The Receiver Operating Characteristic (ROC) curve is commonly used to examine
the trade-off between the detection of true positives, while avoiding the false
positives.
The characteristics of a typical ROC diagram are depicted in the following plot.
Curves are defined on a plot with the proportion of true positives on the vertical axis
and the proportion of false positives on the horizontal axis. Because these values are
equivalent to sensitivity and (1 – specificity), respectively, the diagram is also known
as a sensitivity/specificity plot:
```{r}
plot(sms_roc, main = "ROC curve for SMS spam filter", col = "blue", lwd = 2, legacy.axes = TRUE)
```
The AUC for the SMS classifier is 0.98, which is extremely high. But how do we know whether the model is just as likely to perform well for another dataset? In order to answer such questions, we need to have a better understanding of how far we can extrapolate a model's predictions beyond the test data.
Compare to kNN 
```{r}
sms_results_knn <- read.csv("sms_results_knn.csv")
sms_roc_knn <- roc(sms_results$actual_type, sms_results_knn$p_spam)
plot.new()
plot(sms_roc_knn, main = "ROC curve for KNN SMS spam filter",col = "red", lwd = 2, add = TRUE)
```
Calculate AUC for Naive Bayes and kNN
```{r}
auc(sms_roc)
auc(sms_roc_knn)
```
Estimating Future Performance and partitioning data. The resubstitution error is not a very useful marker of future performance. For example, a model that used rote memorization to perfectly classify every training
instance with zero resubstitution error would be unable to generalize its predictions to data it has never seen before. For this reason, the error rate on the training data can be extremely optimistic about a model's future performance.

```{r}
library(caret)
credit <- read.csv("credit.csv")
```
Holdout method using random IDs
```{r}
random_ids <- order(runif(1000))
credit_train <- credit[random_ids[1:500],]
credit_validate <- credit[random_ids[501:750], ]
credit_test <- credit[random_ids[751:1000], ]
```
Using caret function: The caret package provides a createDataPartition() function that will create partitions based on stratified holdout sampling. The code to create a stratified sample of training and test data for the credit dataset is shown in the following commands.
```{r}
in_train <- createDataPartition(credit$default, p = 0.75, list = FALSE)
credit_train <- credit[in_train, ]
credit_test <- credit[-in_train, ]
```
10-fold CV: For the holdout method to result in a truly accurate estimate of the future performance, at no time should the performance on the test dataset be allowed to influence the model. It is easy to unknowingly violate this rule by choosing the best model based upon the results of repeated testing. For example, suppose we built several models on the training data, and selected the one with the highest accuracy on the test data. Because we have cherry-picked the best result, the test performance is not an unbiased measure of the performance on unseen data.
```{r}
folds <- createFolds(credit$default, k = 10)
str(folds)
credit01_test <- credit[folds$Fold01, ]
credit01_train <- credit[-folds$Fold01, ]
```
Automating 10-fold CV for a C5.0 Decision Tree using ```lapply()```
```{r}
library(caret)
library(C50)
library(irr)
credit <- read.csv("credit.csv")
```
Use an older random number generator to match the book: Finally, we will apply a series of identical steps to the list of folds using the ```lapply()```
function. As shown in the following code, because there is no existing function that does exactly what we need, we must define our own function to pass to ```lapply()```. Our custom function divides the credit data frame into training and test data, builds a decision tree using the ```C5.0()``` function on the training data, generates a set of
predictions from the test data, and compares the predicted and actual values using the ```kappa2()``` function:
```{r}
RNGversion("3.5.2") 
set.seed(123)
folds <- createFolds(credit$default, k = 10)

cv_results <- lapply(folds, function(x) {
  credit_train <- credit[-x, ]
  credit_test <- credit[x, ]
  credit_model <- C5.0(default ~ ., data = credit_train)
  credit_pred <- predict(credit_model, credit_test)
  credit_actual <- credit_test$default
  kappa <- kappa2(data.frame(credit_actual, credit_pred))$value
  return(kappa)
})

str(cv_results)
mean(unlist(cv_results))
```

